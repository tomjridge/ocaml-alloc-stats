{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc88e14f",
   "metadata": {},
   "source": [
    "# ocaml-alloc-stats notebook\n",
    "\n",
    "This notebook allows you to read a \".ctf\" common trace format file containing a memtrace trace\n",
    "of allocations from which you can plot some histograms, and perform some simple statistical analysis.\n",
    "\n",
    "Sample ctf files are uploaded to a shared google drive folder here: https://drive.google.com/drive/folders/1ce5LJ7vEgnaPVgtgUeJ23jHG9vp4K50k?usp=share_link\n",
    "\n",
    "In the cell below you can specify the ctf file you wish to work with. The file `irmin-replay.ctf` should be present in the git repo and serves as a good example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f643edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctf_file = \"irmin-replay.ctf\"\n",
    "\n",
    "# The following flag controls whether we discard allocations that don't survive the minor \n",
    "# heap. These allocations are almost free, so it usually makes sense to discard them when\n",
    "# focusing on the behaviour of the major heap allocator. FIXME TODO\n",
    "ignore_minor_only_allocations = True\n",
    "\n",
    "debug=False # include debug output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de52b6",
   "metadata": {},
   "source": [
    "Check that the ctf and executables are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c556f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, files are present: ./dump_trace.exe irmin-replay.ctf\n"
     ]
    }
   ],
   "source": [
    "# from https://data36.com/plot-histogram-python-pandas/\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import csv # reading CSV format trace files\n",
    "import subprocess # for calling ocaml executables\n",
    "import math \n",
    "import os.path\n",
    "import json\n",
    "import collections\n",
    "\n",
    "\n",
    "# check that a file we rely on is present\n",
    "def file_exists(name):\n",
    "    return os.path.exists(name)\n",
    "\n",
    "dump_exe = \"./dump_trace.exe\"\n",
    "\n",
    "assert file_exists(dump_exe)\n",
    "assert file_exists(ctf_file)\n",
    "print(\"OK, files are present:\", dump_exe, ctf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35178302",
   "metadata": {},
   "source": [
    "Run the executable, store results in a temporary file, and return the name of the temporary file. An alternative would be to read the .ctf directly in Python, but according to \n",
    "https://babeltrace.org/docs/v2.0/python/bt2/ the Python API is undocumented (although \n",
    "probably not too hard to figure out if the current approach is deemed too ugly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef99210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE converted ctf placed in file irmin-replay.ctf.dump\n"
     ]
    }
   ],
   "source": [
    "def convert_ctf_to_dump(filename):\n",
    "    # translate to lookahead format\n",
    "    outfile1 = filename+\".dump\"\n",
    "    subprocess.run([dump_exe,ctf_file,outfile1],check=True)\n",
    "    return outfile1\n",
    "\n",
    "the_ctf_dump = convert_ctf_to_dump(ctf_file)\n",
    "\n",
    "print(\"NOTE converted ctf placed in file \"+the_ctf_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fc0754",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m the_raw_trace \u001b[38;5;241m=\u001b[39m read_dump(the_ctf_dump)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(the_raw_trace[0:10])\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ignore_minor_only_allocations\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# remove allocs that never make it out of the minor heap (they are mostly noise); \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# remove promotes; keep collects for major allocs\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_minor_allocs\u001b[39m(trace):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now we can read the dumped file as plain csv\n",
    "\n",
    "if ignore_minor_only_allocations: \n",
    "    if debug: print(\"NOTE we are ignoring minor-only allocations\")\n",
    "else: \n",
    "    if debug: print(\"NOTE we are including minor-only allocations\")\n",
    "\n",
    "# read dump, retain all info, return as list\n",
    "def read_dump(fn):\n",
    "    xs = []\n",
    "    with open(fn) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        for row in reader:\n",
    "            id = row[0]\n",
    "            entry_type = row[1]\n",
    "            assert (entry_type in [\"A\",\"a\",\"AE\",\"P\",\"C\"])\n",
    "            entry = {}\n",
    "            if entry_type in [\"A\",\"a\",\"AE\"]:\n",
    "                sz = int(row[2])\n",
    "                entry={'id':id,'type':entry_type,'sz':sz}\n",
    "            else: entry={'id':id,'type':entry_type}\n",
    "            xs.append(entry)\n",
    "    return xs\n",
    "\n",
    "the_raw_trace = read_dump(the_ctf_dump)\n",
    "# print(the_raw_trace[0:10])\n",
    "\n",
    "assert ignore_minor_only_allocations\n",
    "\n",
    "# remove allocs that never make it out of the minor heap (they are mostly noise); \n",
    "# remove promotes; keep collects for major allocs\n",
    "def remove_minor_allocs(trace):\n",
    "    res = [] # result trace\n",
    "    in_minor = {} # dict of (id -> size) of allocs currently in the minor heap\n",
    "    for x in trace:\n",
    "        id = x['id']\n",
    "        if x['type'] == \"a\":\n",
    "            in_minor[id] = x['sz']\n",
    "        if x['type'] == \"A\" or x['type'] == \"AE\":\n",
    "            res.append(x)\n",
    "        if x['type'] == \"C\":\n",
    "            if id in in_minor.keys(): pass # drop event\n",
    "            else: res.append(x) # otherwise add to result                \n",
    "        if x['type'] == \"P\":\n",
    "            sz = in_minor[id]\n",
    "            # convert to a major alloc at time of promote            \n",
    "            del in_minor[id]\n",
    "            res.append({'id':id,'type':\"A\",'sz':sz})\n",
    "    return res\n",
    "        \n",
    "the_trace = remove_minor_allocs(the_raw_trace)\n",
    "\n",
    "# from this point, we only need alloc sizes so we can drop ids and collect events\n",
    "the_trace = [ x['sz'] for x in the_trace if x['type']== \"A\" or x['type'] == \"AE\"]\n",
    "\n",
    "# print(the_trace[0:10])\n",
    "print(\"Read\",len(the_trace),\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_allocs_by_size(list_of_allocs):\n",
    "    # maintain a dict, mapping allocation size to number of allocs\n",
    "    allocs = dict()\n",
    "    # and a simple list of alloc sizes\n",
    "    for sz in list_of_allocs:\n",
    "        if sz in allocs:\n",
    "            allocs[sz] = allocs[sz]+1\n",
    "        else:\n",
    "            allocs[sz] = 1\n",
    "    # convert to OrderedDict for readability\n",
    "    allocs = collections.OrderedDict(sorted(allocs.items()))\n",
    "    return allocs\n",
    "\n",
    "the_allocs_by_size = group_allocs_by_size(the_trace)\n",
    "\n",
    "# for easy debugging, we write the_allocs_by_size to a file\n",
    "with open(\"tmp.json\",\"w\") as tmp_json:\n",
    "    json.dump(the_allocs_by_size,tmp_json)\n",
    "    print(\"NOTE wrote the_allocs_by_size to tmp.json\")\n",
    "\n",
    "if debug: the_allocs_by_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7af65c",
   "metadata": {},
   "source": [
    "# Histogram test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://www.tutorialspoint.com/numpy/numpy_histogram_using_matplotlib.htm\n",
    "# and https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "_ = plt.hist(np.array(the_trace),range=(0,32),bins=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bee574",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(the_allocs_by_size.items())) # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bfe9b",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(the_trace),range=(0,32),bins=32) \n",
    "_ = plt.title(\"Small allocations, 0 <= sz < 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ab543",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(the_trace),range=(32,256),bins=100) \n",
    "_ = plt.title(\"Medium allocations, 32 <= sz < 256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(the_trace),range=(256,10000),bins=100) \n",
    "_ = plt.title(\"Large allocations, 256 <= sz < 10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [ x for x in the_trace if x > 10000 ]\n",
    "plt.hist(np.array(xs),bins=100) \n",
    "_ = plt.title(\"Large allocations, sz > 10000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad07285",
   "metadata": {},
   "source": [
    "# Calculation of some basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474007e",
   "metadata": {},
   "source": [
    "## statistic: percentage of large allocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum allocs.items; split into \"small\" vs \"large\" allocs\n",
    "def compute_ratio_small_large(max_small_sz,allocs):\n",
    "    def is_small(sz): return sz <= max_small_sz\n",
    "    total_small_allocs = sum([ v for (k,v) in allocs.items() if is_small(k) ])\n",
    "    total_large_allocs = sum([ v for (k,v) in allocs.items() if not is_small(k) ])\n",
    "    percentage_large = 100.0 * total_large_allocs / (total_small_allocs+total_large_allocs)\n",
    "    total = total_small_allocs+total_large_allocs\n",
    "    return { \"total_small_allocs\":total_small_allocs,\n",
    "            \"total_large_allocs\": total_large_allocs,\n",
    "            \"total\":total,\n",
    "           \"percentage_large\": percentage_large}\n",
    "\n",
    "x = compute_ratio_small_large(256,the_allocs_by_size)\n",
    "\n",
    "print(\"Percentage large allocs of total: \", x[\"percentage_large\"] , \"(total is \"+str(x[\"total\"])+\")\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82907d0e",
   "metadata": {},
   "source": [
    "## statistic: mean, variance, standard deviation, max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b566888",
   "metadata": {},
   "source": [
    "We calculate the mean and standard deviation for allocs grouped by size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocs_grouped_by_size should be a map from sz -> count, where count is\n",
    "# the number of allocations of the given size\n",
    "def compute_stats(allocs):\n",
    "    total_count = sum([count_sz for (sz,count_sz) in allocs.items()])\n",
    "    mean = sum([ sz*count_sz for (sz,count_sz) in allocs.items()]) / total_count\n",
    "    variance = sum( [count_sz * (sz - mean)**2 for (sz,count_sz) in allocs.items()] ) / total_count\n",
    "    standard_deviation = math.sqrt(variance) # NOTE sample standard deviation\n",
    "    max_ = max([ sz for (sz,_) in allocs.items()])\n",
    "    return { \"total_count\":total_count, \n",
    "           \"mean\":mean,\n",
    "           \"variance\":variance,\n",
    "            \"standard_deviation\": standard_deviation,\n",
    "           \"max\":max_}\n",
    "    \n",
    "def test():\n",
    "    # https://en.wikipedia.org/wiki/Standard_deviation\n",
    "    xs = { 2:1, 4:3, 5:2, 7:1, 9:1 }\n",
    "    stats = compute_stats(xs)\n",
    "    expected = {'total_count': 8, 'mean': 5.0, 'variance': 4.0, 'standard_deviation': 2.0, 'max': 9}\n",
    "    assert (stats == expected) \n",
    "        \n",
    "test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_stats(the_allocs_by_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c48235",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL: Calculation of stats for segments of the trace\n",
    "\n",
    "We want to check that the stats are roughly similar for different parts of the trace. For example, are the stats for the first half of the trace similar to those of the second? Essentially we are trying to identify if there are points in the trace where the allocation behaviour changes, see e.g. https://en.wikipedia.org/wiki/Change_detection. There are of course many many ways one might try to decide whether there is a change in allocation behaviour. Comparing stats is one very simple approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0531d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide a list of allocations into n parts, and compute statistics for each part\n",
    "def compute_stats_for_sublists(allocs,n):\n",
    "    len_part = int(math.ceil(len(allocs)/n))\n",
    "    parts = [ allocs[i:i+len_part] for i in range(0,len(allocs),len_part) ]\n",
    "    grouped = [ group_allocs_by_size(part) for part in parts]\n",
    "    stats = [ compute_stats(group) for group in grouped]\n",
    "    return stats # array of stats\n",
    "\n",
    "compute_stats_for_sublists(the_trace,2)\n",
    "# NOTE standard deviations tend to differ considerably, \n",
    "# due to the presence of a single large allocation initially (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = compute_stats_for_sublists(the_trace,10)\n",
    "print(\"Part size: \"+str(zs[0]['total_count'])) # see how large each part is\n",
    "\n",
    "sds = [ stat['standard_deviation'] for stat in zs]\n",
    "sds # look at standard deviations\n",
    "# NOTE the standard deviation seems hugely different in different parts of the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the std-devs differ in different parts of the trace, we might like to \n",
    "# discard the first n and last m of the trace, and then compute the stats for the parts\n",
    "# (based on the idea that there is an initial phase, a middle phase, and a final phase).\n",
    "\n",
    "xs2 = the_trace[100:-100] # drop first and last 100\n",
    "zs = compute_stats_for_sublists(xs2,2)\n",
    "sds = [ stat['standard_deviation'] for stat in zs]\n",
    "sds # look at standard deviations\n",
    "# NOTE so even with this hack of ignoring the first and last 100 allocs, the allocations \n",
    "# in the first half seem different from those in the second. Perhaps irmin-replay does\n",
    "# allocate differently in the two parts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs2) \n",
    "# One issue is that \"most\" of the allocations are small, presumably short lived.\n",
    "# Then the std-dev is really moved by some rare large allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89403c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs3 = [ alloc for alloc in xs2 if alloc > 256] # allocs > 256 words\n",
    "xs3\n",
    "compute_stats_for_sublists(xs3,2)\n",
    "# NOTE still huge difference between sd in first half and second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One thing we could do is count the number of \"large\" allocs in each part of the trace\n",
    "# allocs is a list of allocation sizes; n is the number of parts \n",
    "# NOTE this ignores the actual size of each large allocation, and similarly treats all\n",
    "# small allocations as equals\n",
    "def count_large_allocs(allocs,n):\n",
    "    len_part = int(math.ceil(len(allocs)/n))\n",
    "    parts = [ allocs[i:i+len_part] for i in range(0,len(allocs),len_part) ]\n",
    "    counts = [ len([x for x in part if x > 256]) for part in parts]\n",
    "    return counts\n",
    "\n",
    "count_large_allocs(the_trace,10)\n",
    "# irmin-replay.ctf, parts = 10: [1, 3, 1, 0, 4, 1, 1, 0, 0, 1]\n",
    "# It tells us something (5th part of trace has 4 large allocations!), but it would be nice\n",
    "# to have a 2-phase trace, where allocation behaviour changed, and see if this approach \n",
    "# gives us a sensible signal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09d247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
